{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKSTLF2BX6jH"
   },
   "source": [
    "### ECON 441B HW-Week 7\n",
    "By: George Musabandesu  \n",
    "Due: Friday, Feb 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "Q2A8TGhKm3i5"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7E9HEMJSX-3T"
   },
   "source": [
    "# 1.) Set up OpenAI and the enviornment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "4zwwdkZDYDZN"
   },
   "outputs": [],
   "source": [
    "\n",
    "api_key = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "8IiKS0snlpYP"
   },
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key = openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOXc5_BTm9HP"
   },
   "source": [
    "# 2.) Use the wikipedia api to get a function that pulls in the text of a wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['API_URL',\n",
       " 'BeautifulSoup',\n",
       " 'Decimal',\n",
       " 'DisambiguationError',\n",
       " 'HTTPTimeoutError',\n",
       " 'ODD_ERROR_MESSAGE',\n",
       " 'PageError',\n",
       " 'RATE_LIMIT',\n",
       " 'RATE_LIMIT_LAST_CALL',\n",
       " 'RATE_LIMIT_MIN_WAIT',\n",
       " 'RedirectError',\n",
       " 'USER_AGENT',\n",
       " 'WikipediaException',\n",
       " 'WikipediaPage',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'cache',\n",
       " 'datetime',\n",
       " 'debug',\n",
       " 'donate',\n",
       " 'exceptions',\n",
       " 'geosearch',\n",
       " 'languages',\n",
       " 'page',\n",
       " 'random',\n",
       " 're',\n",
       " 'requests',\n",
       " 'search',\n",
       " 'set_lang',\n",
       " 'set_rate_limiting',\n",
       " 'set_user_agent',\n",
       " 'stdout_encode',\n",
       " 'suggest',\n",
       " 'summary',\n",
       " 'sys',\n",
       " 'time',\n",
       " 'timedelta',\n",
       " 'unicode_literals',\n",
       " 'util',\n",
       " 'wikipedia']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "-v7OYamHlrEB"
   },
   "outputs": [],
   "source": [
    "page_titles = ['Artificial intelligence','UCLA']\n",
    "page_title = page_titles[0]\n",
    "search_results = wikipedia.search(page_title)\n",
    "page = wikipedia.page(search_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page.content\n",
    "def get_wikipedia_content(page_title):\n",
    "    search_results = wikipedia.search(page_title)\n",
    "    page = wikipedia.page(search_results[0])\n",
    "    return(page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "TgY2FkTdmhTH"
   },
   "outputs": [],
   "source": [
    "content = get_wikipedia_content(page_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9aruncMmubX"
   },
   "source": [
    "# 3.) Build a chatgpt bot that will analyze the text given and try to locate any false info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "chat_completions = client.chat.completions.create(\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"I am giving you an article from wikipedia.\\\n",
    "        Please concisely list only the false information found. If there is no false \\\n",
    "        information only return 'DONE'\"},\n",
    "        {\"role\": \"user\", \"content\": content[:8180]}]\n",
    ")\n",
    "print(chat_completions.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "Bmai3B6Dmw3O"
   },
   "outputs": [],
   "source": [
    "def chatgpt_error_correction(text):\n",
    "    chat_completions = client.chat.completions.create(\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"I am giving you an article from wikipedia.\\\n",
    "    Please concisely list only the false information found\"},\n",
    "    {\"role\": \"user\", \"content\": text[:8180]}]\n",
    "    )\n",
    "    print(chat_completions.choices[0].message.content)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPw5LyPEobmk"
   },
   "source": [
    "# 4.) Make a for loop and check a few wikipedia pages and return a report of any potentially false info via wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "V7cuhML2ocGn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________Artificial intelligence\n",
      "- Artificial intelligence was founded as an academic discipline in 1956\n",
      "- The field went through multiple cycles of optimism, followed by periods of disappointment and loss of funding, known as AI winter\n",
      "- The AI spring period occurred in the early 2020s\n",
      "____________UCLA\n",
      "- UCLA was not established in 1881 as a normal school then known as the southern branch of the California State Normal School which later evolved into San Jos√© State University. \n",
      "- It was not transferred to the University of California in 1919 becoming the Southern Branch of UC.\n"
     ]
    }
   ],
   "source": [
    "page_titles = ['Artificial intelligence','UCLA']\n",
    "for page_title in page_titles:\n",
    "    try:\n",
    "        print(\"____________\" + page_title)\n",
    "        content = get_wikipedia_content(page_title)\n",
    "        chatgpt_error_correction(content)\n",
    "    except:\n",
    "        print(\"ERROR\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
